---
title: It all boils down to video games
description: From simulation theory to reinforcement learning to self improvement, it's all video games.
date: "2024-03-03"
---

Warning: This is more of a philosophical shitpost than anything.

> Life, is Roblox. - DJ Khaled

Growing up, I was addicted to video games. Did I care? Not at all, obviously I mean I was still in 8th grade. It wasn't only me, it was all of my friends and what felt like the entire Internet at the time. What made them so addicting was the idea that by investing your time you could actually get better at the game, which meant it only became more fun. This is no surprise I mean looking back, they are exactly made to be this way. Clearly this is also what makes any irl game fun.

But what makes video games interesting is that within these virtual words, the creators can invent brand new sandbox environments, engineered to feel rewarding as you progress. In a video game, you aren't restricted to the laws of physics and resources of the physical world's regular games. Despite a set of rules and ways of interacting through a keyboard being completely made up, you can still step into a new environment and do something in a particular way to win.

One of the biggest machine learning techniques called reinforcement learning (RL) essentially uses this concept of creating a sandbox environment and defining what it means to win to teach computers how to play. It turns out that if you can get the same criteria for what makes video games fun for us: 1) new, exciting game rules and 2) dopamine hits when you win, you can begin to seriously teach it how to win. These two steps map to the two core steps of RL: 1) a defined policy on how to make decisions and moves. 2) a reward function that gives a "feeling" of winning when making a good move. So not by giving it step by step rules, but by putting it in this new environment, computers can learn how to win games on their own.

This technique allows us to teach computers how to get really good at games like pong, or even [drive a car](https://comma.ai). But what if you were able to somehow define these two core concepts for the entire world? Just thinking about it, isn't the world just a really really complicated game with fundamental rules like physics and chemistry?

Who knows, but OpenAI's newest model Sora takes on some parts of this question as a model that can ["simulate the real world"](https://openai.com/sora). This may sound like some marketing garbage, but what if the model really can simulate parts of our world? What if it could actually simulate the underlying physics of the motion in the videos as they are generated?

Using Sora as an example, it feels like every new model being trained, with cleaner and more extensive data is attempting at defining a *world* model. Given such a dataset it seems at least possible to think a genuine sandbox environment and defined game rules could exist on such a scale.

Therefore, it feels like we are building towards an odd full circle. What initially started at video games, led to RL, which led to incredible advances in
ML with the transformer, may end up at a true world model. Which could construct a
virtual environment/sandbox wrapping our world. A true simulation. This would be the greatest video game ever built.
But, linking back to my own and many people's regular video game addictions, what will this next generation world model cost?
